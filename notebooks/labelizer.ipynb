{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80eb7c9",
   "metadata": {},
   "source": [
    "# **Label every cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ff219",
   "metadata": {},
   "source": [
    "### **1. Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92132d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\PROFESIONAL\\pmb-clusterign\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np, pandas as pd, re, random\n",
    "from tqdm.auto import tqdm\n",
    "# Level up one level directory to add app the the allowed routes\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from app.utils.base_dir import BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab24d2",
   "metadata": {},
   "source": [
    "### **2. Load the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d644305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\PROFESIONAL\\pmb-clusterign\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rofer\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "#embed_model = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # using a mini model for performance\n",
    "kw_model = KeyBERT(model=embed_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d883f",
   "metadata": {},
   "source": [
    "### **3. Extract the keywords for cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107116c",
   "metadata": {},
   "source": [
    "**3.1 Function to extract the keywords per cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_keybert_clusterwise(\n",
    "        texts,  # the corpus of the cluster\n",
    "        top_n=10,       #number of keywords that return\n",
    "        keyphrase_ngram_range=(1,2),    # use bigrams\n",
    "        use_mmr=True,   # avoid redudant words\n",
    "        diversity=0.6,  # similitud and viriety\n",
    "        nr_candidates=20,\n",
    "        stop_words='english',\n",
    "    ):\n",
    "    \n",
    "    kws = kw_model.extract_keywords(texts,\n",
    "        keyphrase_ngram_range = keyphrase_ngram_range,\n",
    "        stop_words = stop_words,\n",
    "        top_n = top_n,\n",
    "        use_mmr = use_mmr,\n",
    "        diversity = diversity,\n",
    "        nr_candidates = nr_candidates)\n",
    "    # kws -> list of (keyword, score)\n",
    "    kws = [k for k,_ in kws]\n",
    "    return kws\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c795a",
   "metadata": {},
   "source": [
    "**3.2 Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345a278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clean_data_path = BASE_DIR/'data'/'processed'/'noCleanProcessedData.json' \n",
    "clusters_path = BASE_DIR/'data'/'processed'/'clusters.json' \n",
    "\n",
    "no_clean_data_array = []\n",
    "no_clean_data = {}\n",
    "clusters = {}\n",
    "with open(no_clean_data_path, 'r') as f:\n",
    "    no_clean_data_array = json.load(f)\n",
    "\n",
    "with open(clusters_path, 'r') as f:\n",
    "    clusters = json.load(f)\n",
    "\n",
    "def parse_to_dict(data:list):\n",
    "    for element in data:\n",
    "        for k, v in element.items():\n",
    "            no_clean_data[k] = v\n",
    "parse_to_dict(no_clean_data_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf6eb4",
   "metadata": {},
   "source": [
    "**3.3 Get cluster corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cde0df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# like {11 : [\"text 01\", \"text 02\"]}\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m clusterNumber_corpusText = \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      3\u001b[39m MAX_DOCS = \u001b[32m20\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cluster_number, titles_list \u001b[38;5;129;01min\u001b[39;00m clusters.items():\n",
      "\u001b[31mNameError\u001b[39m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "# like {11 : [\"text 01\", \"text 02\"]}\n",
    "clusterNumber_corpusText = defaultdict(str)\n",
    "MAX_DOCS = 20\n",
    "\n",
    "for cluster_number, titles_list in clusters.items():\n",
    "    for i, title in enumerate(titles_list):\n",
    "        i+=1\n",
    "        if i <= MAX_DOCS:\n",
    "            clusterNumber_corpusText[cluster_number] += no_clean_data[title]\n",
    "        else: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1534ec6",
   "metadata": {},
   "source": [
    "**3.2 Extract keywords for all custer's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords():\n",
    "    response = {} # is a dict with {number_cluster:[keyword01, keyword02,...]}\n",
    "    for c_number, list_text in clusterNumber_corpusText.items():\n",
    "        kwrds = extract_keywords_keybert_clusterwise(list_text)\n",
    "        response[c_number] = kwrds \n",
    "    return response\n",
    "\n",
    "res = extract_keywords()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
