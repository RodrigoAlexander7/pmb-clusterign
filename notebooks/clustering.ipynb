{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3720afc9",
   "metadata": {},
   "source": [
    "## Vectorizing and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18777439",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5037bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import hdbscan\n",
    "# Level up one level to add app the the allowed routes\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from app.utils.base_dir import BASE_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc898e",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab76281",
   "metadata": {},
   "source": [
    "open the json and load the text and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42600b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = BASE_DIR/'data'/'processed'/'processedData.json' \n",
    "with open(path, 'r') as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "label = [list(d.keys())[0] for d in docs]\n",
    "text = [list(d.values())[0] for d in docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53001309",
   "metadata": {},
   "source": [
    "check if is the text is none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72530b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Documento 12 es None\n",
      "❌ Documento 17 es None\n",
      "❌ Documento 21 es None\n",
      "❌ Documento 35 es None\n",
      "❌ Documento 36 es None\n",
      "❌ Documento 42 es None\n",
      "❌ Documento 43 es None\n",
      "❌ Documento 45 es None\n",
      "❌ Documento 46 es None\n",
      "❌ Documento 48 es None\n",
      "❌ Documento 49 es None\n",
      "❌ Documento 54 es None\n",
      "❌ Documento 68 es None\n",
      "❌ Documento 70 es None\n",
      "❌ Documento 71 es None\n",
      "❌ Documento 72 es None\n",
      "❌ Documento 75 es None\n",
      "❌ Documento 87 es None\n",
      "❌ Documento 91 es None\n",
      "❌ Documento 92 es None\n",
      "❌ Documento 113 es None\n",
      "❌ Documento 119 es None\n",
      "❌ Documento 120 es None\n",
      "❌ Documento 121 es None\n",
      "❌ Documento 123 es None\n",
      "❌ Documento 125 es None\n",
      "❌ Documento 127 es None\n",
      "❌ Documento 142 es None\n",
      "❌ Documento 161 es None\n",
      "❌ Documento 168 es None\n",
      "❌ Documento 171 es None\n",
      "❌ Documento 174 es None\n",
      "❌ Documento 175 es None\n",
      "❌ Documento 177 es None\n",
      "❌ Documento 181 es None\n",
      "❌ Documento 183 es None\n",
      "❌ Documento 192 es None\n",
      "❌ Documento 211 es None\n",
      "❌ Documento 217 es None\n",
      "❌ Documento 223 es None\n",
      "❌ Documento 227 es None\n",
      "❌ Documento 228 es None\n",
      "❌ Documento 232 es None\n",
      "❌ Documento 237 es None\n",
      "❌ Documento 238 es None\n",
      "❌ Documento 247 es None\n",
      "❌ Documento 248 es None\n",
      "❌ Documento 256 es None\n",
      "❌ Documento 258 es None\n",
      "❌ Documento 260 es None\n",
      "❌ Documento 261 es None\n",
      "❌ Documento 267 es None\n",
      "❌ Documento 268 es None\n",
      "❌ Documento 277 es None\n",
      "❌ Documento 279 es None\n",
      "❌ Documento 283 es None\n",
      "❌ Documento 319 es None\n",
      "❌ Documento 320 es None\n",
      "❌ Documento 333 es None\n",
      "❌ Documento 334 es None\n",
      "❌ Documento 339 es None\n",
      "❌ Documento 341 es None\n",
      "❌ Documento 342 es None\n",
      "❌ Documento 345 es None\n",
      "❌ Documento 348 es None\n",
      "❌ Documento 350 es None\n",
      "❌ Documento 351 es None\n",
      "❌ Documento 352 es None\n",
      "❌ Documento 354 es None\n",
      "❌ Documento 362 es None\n",
      "❌ Documento 370 es None\n",
      "❌ Documento 371 es None\n",
      "❌ Documento 380 es None\n",
      "❌ Documento 406 es None\n",
      "❌ Documento 409 es None\n",
      "❌ Documento 413 es None\n",
      "❌ Documento 415 es None\n",
      "❌ Documento 422 es None\n",
      "❌ Documento 423 es None\n",
      "❌ Documento 424 es None\n",
      "❌ Documento 480 es None\n",
      "❌ Documento 485 es None\n",
      "❌ Documento 502 es None\n",
      "❌ Documento 555 es None\n",
      "❌ Documento 573 es None\n",
      "❌ Documento 590 es None\n",
      "❌ Documento 596 es None\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(text):\n",
    "    if doc is None:\n",
    "        print(f\"Documento {i} es None\")\n",
    "    elif not isinstance(doc, str):\n",
    "        print(f\"Documento {i} no es str: {type(doc)} -> {repr(doc)}\")\n",
    "    elif not doc.strip():\n",
    "        print(f\"Documento {i} está vacío\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41199a",
   "metadata": {},
   "source": [
    "vectorizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efb6b709",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m     vectorizer = TfidfVectorizer(\n\u001b[32m      3\u001b[39m         max_features= \u001b[32m10000\u001b[39m,  \u001b[38;5;66;03m# the max num of words for the total corpus \u001b[39;00m\n\u001b[32m      4\u001b[39m         max_df=\u001b[32m0.85\u001b[39m,    \u001b[38;5;66;03m#delete if the word apears on the 85% of docs\u001b[39;00m\n\u001b[32m      5\u001b[39m         min_df=\u001b[32m0.01\u001b[39m,    \u001b[38;5;66;03m#delete if tye word aperas just on the 1% of docs\u001b[39;00m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer.fit_transform(text_corpus)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m TF_vector = \u001b[43mTF_IDF_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(TF_vector) \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mTF_IDF_vectorizer\u001b[39m\u001b[34m(text_corpus)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mTF_IDF_vectorizer\u001b[39m(text_corpus: \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m      2\u001b[39m     vectorizer = TfidfVectorizer(\n\u001b[32m      3\u001b[39m         max_features= \u001b[32m10000\u001b[39m,  \u001b[38;5;66;03m# the max num of words for the total corpus \u001b[39;00m\n\u001b[32m      4\u001b[39m         max_df=\u001b[32m0.85\u001b[39m,    \u001b[38;5;66;03m#delete if the word apears on the 85% of docs\u001b[39;00m\n\u001b[32m      5\u001b[39m         min_df=\u001b[32m0.01\u001b[39m,    \u001b[38;5;66;03m#delete if tye word aperas just on the 1% of docs\u001b[39;00m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_corpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\PROFESIONAL\\pmb-clusterign\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2105\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2099\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2100\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2101\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2102\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2103\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2104\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2105\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2108\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\PROFESIONAL\\pmb-clusterign\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\PROFESIONAL\\pmb-clusterign\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1369\u001b[39m             warnings.warn(\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1374\u001b[39m             )\n\u001b[32m   1375\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1380\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\PROFESIONAL\\pmb-clusterign\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1264\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1263\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1266\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\PROFESIONAL\\pmb-clusterign\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:104\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         doc = \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    106\u001b[39m         doc = tokenizer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mw:\\PROFESIONAL\\pmb-clusterign\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:62\u001b[39m, in \u001b[36m_preprocess\u001b[39m\u001b[34m(doc, accent_function, lower)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mapply to a document.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33;03m    preprocessed string\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     doc = \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     doc = accent_function(doc)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "def TF_IDF_vectorizer(text_corpus: list):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features= 10000,  # the max num of words for the total corpus \n",
    "        max_df=0.85,    #delete if the word apears on the 85% of docs\n",
    "        min_df=0.01,    #delete if tye word aperas just on the 1% of docs\n",
    "    )\n",
    "    return vectorizer.fit_transform(text_corpus)\n",
    "TF_vector = TF_IDF_vectorizer(text)\n",
    "print(TF_vector) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737612f9",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c83dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  5  5 -1 -1 -1\n",
      " -1 -1 -1  5 -1 -1 -1 -1 -1  5 -1 -1 -1 -1  5 -1  5  1 -1 -1 -1 -1  1 -1\n",
      " -1  5  1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1  4 -1 -1  1 -1  5  1 -1 -1 -1  5\n",
      " -1 -1  1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1  5 -1 -1  1 -1 -1  4 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1 -1 -1  5 -1  5 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1  2 -1 -1 -1  4  4 -1\n",
      "  1 -1  1 -1  5 -1 -1 -1 -1  5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  5  2 -1 -1  1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1 -1\n",
      " -1  5  4  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  5 -1 -1 -1  5\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  4 -1 -1 -1  1 -1 -1  5 -1  1  1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  5  5 -1  1 -1  1 -1 -1 -1 -1 -1 -1\n",
      " -1  5 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  2 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  1  1 -1 -1  2 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1  1 -1 -1 -1 -1  5  5 -1  3 -1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  1  5 -1 -1 -1 -1 -1  1 -1  5 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  2 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1  5 -1 -1 -1 -1  1 -1 -1 -1  3 -1 -1 -1  3 -1 -1 -1  3 -1 -1 -1\n",
      " -1 -1 -1 -1  5  2  5 -1  2 -1 -1 -1 -1  2  1 -1 -1 -1  2 -1 -1 -1 -1 -1\n",
      "  4  4  4  4  4 -1  4 -1  4  4 -1  4  4  1  4  4  3  3  1  4  4 -1 -1 -1\n",
      "  4  4  4  4  4  4  4 -1  4  4 -1 -1  4  1 -1  4  4 -1 -1 -1  4  4  4  1\n",
      " -1  0 -1  0 -1 -1 -1 -1 -1 -1 -1  3 -1 -1 -1  5  5  0 -1  4 -1 -1  5 -1\n",
      " -1 -1 -1 -1 -1 -1  3 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0  4 -1 -1\n",
      " -1 -1]\n"
     ]
    }
   ],
   "source": [
    "def cluster_hdbscan(TF_vector):\n",
    "    clusterer = hdbscan.HDBSCAN()\n",
    "    # return a object like [0,1,5,0,1,5,6,9,0] where every number in order is the number of cluster of the document\n",
    "    return clusterer.fit_predict(TF_vector)\n",
    "    \n",
    "print(cluster_hdbscan(TF_vector))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
