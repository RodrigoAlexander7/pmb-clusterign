{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3720afc9",
   "metadata": {},
   "source": [
    "# **Vectorizing and Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18777439",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5037bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import hdbscan\n",
    "# Level up one level directory to add app the the allowed routes\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from app.utils.base_dir import BASE_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc898e",
   "metadata": {},
   "source": [
    "## **1. Vectorizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab76281",
   "metadata": {},
   "source": [
    "open the json and load the text and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42600b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = BASE_DIR/'data'/'processed'/'processedData.json' \n",
    "with open(path, 'r') as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "label = [list(d.keys())[0] for d in docs]\n",
    "text = [list(d.values())[0] for d in docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53001309",
   "metadata": {},
   "source": [
    "check if is the text is none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72530b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(text):\n",
    "    if doc is None:\n",
    "        print(f\"Documento {i} es None\")\n",
    "    elif not isinstance(doc, str):\n",
    "        print(f\"Documento {i} no es str: {type(doc)} -> {repr(doc)}\")\n",
    "    elif not doc.strip():\n",
    "        print(f\"Documento {i} está vacío\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41199a",
   "metadata": {},
   "source": [
    "### **1.1 Vectorizing the text with TF_IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8feef5",
   "metadata": {},
   "source": [
    "### **1.1 Vectorizing the text with TF_IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TF_IDF_vectorizer(text_corpus: list):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        #max_features= 10000,  # the max num of words for the total corpus \n",
    "        max_df=0.95,    #delete if the word apears on the 85% of docs\n",
    "        min_df=0.001,    #delete if tye word aperas just on the 0.1% of docs\n",
    "    )\n",
    "    return vectorizer.fit_transform(text_corpus)\n",
    "TF_vector = TF_IDF_vectorizer(text)\n",
    "print(TF_vector) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84f60a",
   "metadata": {},
   "source": [
    "## **2. Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737612f9",
   "metadata": {},
   "source": [
    "### **HDBSACAN**\n",
    "More strict than k means\n",
    "- The documments need to be very closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c83dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 43 41 -1 -1 40 43 -1 -1 -1 -1  4 -1 -1 48 -1 -1 -1 -1 -1 -1 -1 44 39\n",
      " -1  3 -1 -1 33 23 -1 -1 -1 -1 -1 -1  0  0  0  2  2 -1  2 42 -1 39 -1 43\n",
      " -1  1  1  1 -1 45 48 -1 45 45 -1 -1 -1 -1 -1 -1 38 -1 -1 -1 -1 18 -1 -1\n",
      " 18  5  5  5  5 -1 -1  5 -1 15 15 -1 33 39 33 -1 -1 -1 33 33 37 -1 30 30\n",
      " -1 -1 -1 -1 -1 32 -1 32  1  1 -1 22 34 -1 34 22 22 34 -1 -1 -1  4 47  4\n",
      " -1 -1 -1 -1 47 17 -1 -1  9 -1  9  9 -1 -1 -1 23 -1 25 -1 46 46 46 13 -1\n",
      " -1 47 -1 -1 47 43 43 19 10 43 26 -1 26 -1  7 -1  7 23  7 -1  7  7 44 -1\n",
      " -1 39 -1 -1 -1 -1 43 -1 -1 43 23 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 36 -1 -1 36 -1 39 -1 -1 -1 43 -1 10 10 -1 47 47 47 47 47 -1 47\n",
      " -1 -1 -1 -1 12 -1 -1 -1 -1 -1 -1 -1 -1 32 32 32 32 40 32 32 -1 33 -1 14\n",
      " 41 -1 -1 41 35 -1 41 41 41 -1 33 41 -1 -1 33 -1 -1 16 -1 -1 46 -1 -1 -1\n",
      "  8  8  8  8 21 35 46 31 31 31 31 -1 48  3 -1 -1 -1 -1 -1 -1 24 -1 -1 -1\n",
      " 23 15  3 -1 24 24 -1 -1 18 -1 41 41 -1 46 -1 -1 47 10 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 39 44 -1 40 -1 -1 -1 47 -1 19  1 -1 -1 -1 37\n",
      " -1 -1 13 13 -1 -1 38 38 -1 26 -1 42 -1 42 42 38 -1 -1 37 -1 37 43 43 40\n",
      " -1 -1 -1 15 -1 -1 18 -1 -1 -1 -1  3  3  3  3  3 21 19 -1 -1 -1 35 43 43\n",
      " 43 43 43 43 43 43 43 43 43 -1 43 43 43 43 -1 43 43 25 35 29 -1 37 -1 37\n",
      " -1  6 37 -1 37 -1 -1 -1 37 -1 -1  6 -1 36 10 10 -1 10 36 -1 12 -1 -1 -1\n",
      " 20 -1 36 20 10 28 -1 10 10 20 36 10 -1 11 14 -1 28 -1 27 -1 -1 28 -1 -1\n",
      " -1 -1 16 -1 11 14 27 -1 25 25 -1 27 28 28 28 -1 -1 17 43 17 -1 -1 -1 44\n",
      " -1 -1 47 29 29 12 -1 11 -1 -1 29 29 -1 15 -1 16 -1 -1 -1 -1 43 -1 -1 -1\n",
      " -1 -1 21  6 -1 28 30 -1 41 -1 40]\n"
     ]
    }
   ],
   "source": [
    "def cluster_hdbscan(TF_vector):\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=3,\n",
    "        min_samples=1\n",
    "    )\n",
    "    # return a object like [0,1,5,0,1,5,6,9,0] where every number in order is the number of cluster of the document\n",
    "    return clusterer.fit_predict(TF_vector)\n",
    "    \n",
    "print(cluster_hdbscan(TF_vector))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9fcf4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
